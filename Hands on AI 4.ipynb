{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mWbPLEh9XuFT"
      },
      "source": [
        "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
        "<h2 style=\"color:rgb(0,120,170)\">Unit 5 â€“ Language Modeling with LSTM (Assignment)</h2>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7O0zj9EFXuFZ"
      },
      "source": [
        "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
        "<p><p>This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which might require small code modifications. Most/All of the used functions are imported from the file <code>u5_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u5_utils.py</code> need to be installed.</p></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5guqRaCzXuFb",
        "outputId": "9b6e3cdf-6346-4168-92f8-dd9407e1f09f"
      },
      "outputs": [],
      "source": [
        "!pip install ipdb\n",
        "\n",
        "import u5_utils as u5\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import ipdb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set default plotting style.\n",
        "sns.set()\n",
        "\n",
        "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
        "u5.setup_jupyter()\n",
        "\n",
        "# Check minimum versions.\n",
        "u5.check_module_versions()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JCVsjmjoXuFl"
      },
      "source": [
        "<h2>Language Model Training and Evaluation</h2>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k-ywHGzoXuFn"
      },
      "source": [
        "<h3 style=\"color:rgb(0,120,170)\">Data & Dictionary Preperation</h3>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z-HyvSXBXuFp"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <b>Exercise 1. [20 Points]</b>\n",
        "        <ul>\n",
        "            <li>Setup the data set using the same parameter settings as in the main exercise notebook but with the changes mentioned below.</li>\n",
        "            <li>Change the batch size in the initial parameters to $64$ and observe its effect on the created batches. Explain how the corpora are transformed into batches.</li>\n",
        "            <li>Use a seed of $23$.</li>\n",
        "            <li>For a specific sequence in <code>val_data_splits</code> (e.g., index $15$), print the corresponding words of its first 25 wordIDs.</li>\n",
        "        </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDGFQKVxXuFr",
        "outputId": "83280611-4d80-4e4f-ee54-4eab22ea849e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in dictionary: 10001\n",
            "Train data: number of tokens 929589\n",
            "Validation data: number of tokens 73760\n",
            "Test data: number of tokens 82430\n",
            "\n",
            "Train data split shape: torch.Size([14524, 64])\n",
            "Validation data split shape: torch.Size([1152, 64])\n",
            "Test data batchified shape: torch.Size([1287, 64])\n",
            "Corresponding word IDs:\n",
            "[4535, 1363, 153, 2052, 49, 263, 1021, 746, 25, 393, 1420, 27, 41, 36, 27, 43, 2970, 157, 49, 4191, 869, 3156, 25, 193, 629]\n",
            "Corresponding words:\n",
            "['weekly', 'reports', 'on', 'school', 'and', 'college', 'construction', 'plans', '<eos>', 'market', 'data', '<unk>', 'is', 'a', '<unk>', 'of', 'educational', 'information', 'and', 'provides', 'related', 'services', '<eos>', 'closely', 'held']\n"
          ]
        }
      ],
      "source": [
        "save_path = \"model.pt\" # path to save the final model\n",
        "\n",
        "# Change the batch size in the initial parameters to 64\n",
        "train_batch_size = 64\n",
        "eval_batch_size = 64\n",
        "max_seq_len = 40\n",
        "\n",
        "# Set the random seed\n",
        "torch.manual_seed(23)\n",
        "\n",
        "# Check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# Set up the data set\n",
        "train_corpus = u5.Corpus(\"train.txt\")\n",
        "valid_corpus = u5.Corpus(\"valid.txt\")\n",
        "test_corpus = u5.Corpus(\"test.txt\")\n",
        "\n",
        "# Fill the dictionary with the words from the train corpus\n",
        "dictionary = u5.Dictionary()\n",
        "train_corpus.fill_dictionary(dictionary)\n",
        "ntokens = len(dictionary)\n",
        "print(f\"Number of tokens in dictionary: {ntokens}\")\n",
        "\n",
        "train_data = train_corpus.words_to_ids(dictionary)\n",
        "print(f\"Train data: number of tokens {len(train_data)}\")\n",
        "\n",
        "valid_data = valid_corpus.words_to_ids(dictionary)\n",
        "print(f\"Validation data: number of tokens {len(valid_data)}\")\n",
        "\n",
        "test_data = test_corpus.words_to_ids(dictionary)\n",
        "print(f\"Test data: number of tokens {len(test_data)}\")\n",
        "\n",
        "print()\n",
        "train_data_splits = u5.batchify(train_data, train_batch_size, device)\n",
        "print(f\"Train data split shape: {train_data_splits.shape}\")\n",
        "\n",
        "val_data_splits = u5.batchify(valid_data, eval_batch_size, device)\n",
        "print(f\"Validation data split shape: {val_data_splits.shape}\")\n",
        "\n",
        "test_data_splits = u5.batchify(test_data, eval_batch_size, device)\n",
        "print(f\"Test data batchified shape: {test_data_splits.shape}\")\n",
        "\n",
        "# Access the sequence at index 15 in val_data_splits\n",
        "sequence_index = 15\n",
        "sequence = val_data_splits[:, sequence_index]\n",
        "\n",
        "# Retrieve the first 25 word IDs\n",
        "word_ids = sequence[:25].tolist()\n",
        "\n",
        "# Look up the corresponding words using the dictionary\n",
        "words = [dictionary.idx2word[word_id] for word_id in word_ids]\n",
        "\n",
        "# Print the corresponding word IDs\n",
        "print(\"Corresponding word IDs:\")\n",
        "print(word_ids)\n",
        "\n",
        "# Print the corresponding words\n",
        "print(\"Corresponding words:\")\n",
        "print(words)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hcDiMpXIXuFs"
      },
      "source": [
        "After changing the batch sizes, during training, the training data will be divided into batches, and each batch will contain 64 tokens as\n",
        "it is printed in the code at 'shapes' part. \n",
        "\n",
        "The transformation of corpora into batches involves dividing the data into smaller, manageable chunks for efficient processing during training or evaluation. It is typically done by grouping sequential data points into fixed-size batches based on a specified batch size. The resulting batches enable parallelization and vectorized operations, optimizing computational efficiency and enabling the training or evaluation of models on larger datasets."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bBiQkG8MXuFu"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <b>Exercise 2. [20 Points]</b>\n",
        "        <ul>\n",
        "            <li>Copy the implementation of <code>LM_LSTMModel</code> from the main exercise notebook but make the following changes:</li>\n",
        "            <ul>\n",
        "                <li>Add an integer parameter to <code>LM_LSTMModel</code>'s initialization, called <code>num_layers</code> which indicates the number of (vertically) stacked LSTM blocks. Hint: PyTorch's LSTM implementation directly supports this, so you simply have to set it when creating the LSTM instance (see parameter <code>num_layers</code> in the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">documentation</a>).</li>\n",
        "                <li>Add a new bool parameter to <code>LM_LSTMModel</code>'s initialization, called <code>tie_weights</code>. Extend the implementation of <code>LM_LSTMModel</code> such that if <code>tie_weights</code> is set to <code>True</code>, the model ties/shares the parameters of <code>encoder</code> with the ones of <code>decoder</code>. Consider that <code>encoder</code> and <code>decoder</code> still remain separate components but their parameters are now the same (shared). This process is called <i>weight tying</i>. Feel free to search the internet for relevant resources and implementation hints.</li>\n",
        "            </ul>\n",
        "            <li>Create four models:</li>\n",
        "            <ul>\n",
        "                <li>1 layer and without weight tying</li>\n",
        "                <li>1 layer and with weight tying</li>\n",
        "                <li>2 layers and without weight tying</li>\n",
        "                <li>2 layers and with weight tying</li>\n",
        "            </ul>\n",
        "            <li>Compare the number of parameters of the models and report your observations.</li>\n",
        "        </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l61UoEgXuFw",
        "outputId": "67e36561-ccd8-4fd4-8fe0-6b324913d399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model_1layer_no_tying: LM_LSTMModel(\n",
            "  (encoder): Embedding(10001, 200)\n",
            "  (rnn): LSTM(200, 200)\n",
            "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
            ")\n",
            "Model total trainable parameters: 4332001\n",
            "model_1layer_tying: LM_LSTMModel(\n",
            "  (encoder): Embedding(10001, 200)\n",
            "  (rnn): LSTM(200, 200)\n",
            "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
            ")\n",
            "Model total trainable parameters: 2331801\n",
            "model_2layers_no_tying: LM_LSTMModel(\n",
            "  (encoder): Embedding(10001, 200)\n",
            "  (rnn): LSTM(200, 200, num_layers=2)\n",
            "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
            ")\n",
            "Model total trainable parameters: 4653601\n",
            "model_2layers_tying: LM_LSTMModel(\n",
            "  (encoder): Embedding(10001, 200)\n",
            "  (rnn): LSTM(200, 200, num_layers=2)\n",
            "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
            ")\n",
            "Model total trainable parameters: 2653401\n"
          ]
        }
      ],
      "source": [
        "class LM_LSTMModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, num_layers, tie_weights=False):\n",
        "        super().__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.encoder = torch.nn.Embedding(ntoken, ninp)  # matrix E in the figure\n",
        "        self.rnn = torch.nn.LSTM(ninp, nhid, num_layers=num_layers)\n",
        "\n",
        "        if tie_weights:\n",
        "            self.decoder = torch.nn.Linear(nhid, ntoken)\n",
        "            self.decoder.weight = self.encoder.weight  # tie the weights of encoder and decoder\n",
        "        else:\n",
        "            self.decoder = torch.nn.Linear(nhid, ntoken)  # matrix U in the figure\n",
        "\n",
        "    def forward(self, input, hidden=None, return_logs=True):\n",
        "        emb = self.encoder(input)\n",
        "        hiddens, last_hidden = self.rnn(emb, hidden)\n",
        "\n",
        "        decoded = self.decoder(hiddens)\n",
        "        if return_logs:\n",
        "            y_hat = torch.nn.LogSoftmax(dim=-1)(decoded)\n",
        "        else:\n",
        "            y_hat = torch.nn.Softmax(dim=-1)(decoded)\n",
        "\n",
        "        return y_hat, last_hidden\n",
        "\n",
        "model_1layer_no_tying = LM_LSTMModel(ntokens, 200, 200, num_layers=1, tie_weights=False)\n",
        "model_1layer_tying = LM_LSTMModel(ntokens, 200, 200, num_layers=1, tie_weights=True)\n",
        "model_2layers_no_tying = LM_LSTMModel(ntokens, 200, 200, num_layers=2, tie_weights=False)\n",
        "model_2layers_tying = LM_LSTMModel(ntokens, 200, 200, num_layers=2, tie_weights=True)\n",
        "\n",
        "print(f\"model_1layer_no_tying: {model_1layer_no_tying}\")\n",
        "print(f\"Model total trainable parameters: {sum(p.numel() for p in model_1layer_no_tying.parameters() if p.requires_grad)}\")\n",
        "print(f\"model_1layer_tying: {model_1layer_tying}\")\n",
        "print(f\"Model total trainable parameters: {sum(p.numel() for p in model_1layer_tying.parameters() if p.requires_grad)}\")\n",
        "print(f\"model_2layers_no_tying: {model_2layers_no_tying}\")\n",
        "print(f\"Model total trainable parameters: {sum(p.numel() for p in model_2layers_no_tying.parameters() if p.requires_grad)}\")\n",
        "print(f\"model_2layers_tying: {model_2layers_tying}\")\n",
        "print(f\"Model total trainable parameters: {sum(p.numel() for p in model_2layers_tying.parameters() if p.requires_grad)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V1XDE8A9XuFx"
      },
      "source": [
        "Enabling weight tying reduces the number of parameters in the model, leading to a more parameter-efficient architecture.\n",
        "\n",
        "Increasing the number of layers in the LSTM network significantly increases the total number of trainable parameters.\n",
        "\n",
        "Weight tying can be particularly beneficial when working with limited computational resources or when aiming to reduce overfitting by limiting the model's capacity.\n",
        "\n",
        "The specific values of the trainable parameters may vary depending on the vocabulary size (ntokens), input dimension (200), and hidden dimension (200) used in the model configurations."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcdV_UnIXuFy"
      },
      "source": [
        "<h3 style=\"color:rgb(0,120,170)\">Training and Evaluation</h3>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtzf1XCqXuFz"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <b>Exercise 3. [30 Points]</b>\n",
        "    <ul>\n",
        "        <li>Using the same setup as in the main lecture/exercise notebook, train all four models for $5$ epochs.</li>\n",
        "        <li>Using <code>ipdb</code>, look inside the <code>forward</code> function of <code>LM_LSTMModel</code> during training. Check the forward process from input to output particularly by looking at the shapes of tensors. Report the shape of all tensors used in <code>forward</code>. Try to translate the numbers into batches $B$ and sequence length $L$. For instance, if we know that the batch size is $B=32$, a tensor of shape $(32, 128, 3)$ can be interpreted as a batch of $32$ sequences with $3$ channels of size $L=128$. Thus, this tensor can be translated into $(32, 128, 3) \\rightarrow (B, L, 3)$. Look at the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">official documentation</a> to understand the order of the dimensions.</li>\n",
        "        <li>Evaluate the models. Compare the performances of all four models on the train, validation and test set (for the test set, use the best model according to the respective validation set performance), and report your observations. To do so, create a plot showing the following curves:</li>\n",
        "        <ul>\n",
        "            <li>Loss on each current training batch before every model update step as function of epochs</li>\n",
        "            <li>Loss on the validation set at every epoch</li>\n",
        "        </ul>\n",
        "        <li>Comment on the results!</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdfyeRH1XuF0",
        "outputId": "4ac2ef82-53c0-4a0f-e85c-41278e65198a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 773.19 | loss  4.43 | perplexity    84.08\n",
            "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 773.19 | loss  4.49 | perplexity    89.06\n",
            "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 747.33 | loss  4.75 | perplexity   115.51\n",
            "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 735.39 | loss  4.71 | perplexity   111.57\n",
            "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 752.11 | loss  4.66 | perplexity   105.21\n",
            "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 731.84 | loss  4.54 | perplexity    93.82\n",
            "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 744.93 | loss  4.64 | perplexity   103.37\n",
            "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 757.73 | loss  4.81 | perplexity   122.37\n",
            "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 744.89 | loss  5.47 | perplexity   238.21\n",
            "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 751.42 | loss  5.39 | perplexity   220.21\n",
            "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 739.20 | loss  5.41 | perplexity   224.12\n",
            "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 749.51 | loss  5.38 | perplexity   217.54\n",
            "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 731.56 | loss  5.44 | perplexity   231.01\n",
            "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 748.12 | loss  5.34 | perplexity   209.18\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   0 | time: 280.26s| valid loss  5.42 | valid perplexity   225.84\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 770.14 | loss  5.05 | perplexity   156.53\n",
            "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 747.40 | loss  4.74 | perplexity   114.05\n",
            "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 751.10 | loss  4.77 | perplexity   117.45\n",
            "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 746.95 | loss  4.74 | perplexity   114.37\n",
            "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 740.57 | loss  4.67 | perplexity   106.79\n",
            "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 741.42 | loss  4.55 | perplexity    94.34\n",
            "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 749.64 | loss  4.64 | perplexity   103.85\n",
            "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 743.81 | loss  4.76 | perplexity   117.11\n",
            "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 759.68 | loss  5.18 | perplexity   177.56\n",
            "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 741.28 | loss  5.14 | perplexity   171.15\n",
            "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 746.82 | loss  5.19 | perplexity   178.58\n",
            "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 750.35 | loss  5.16 | perplexity   174.99\n",
            "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 737.92 | loss  5.24 | perplexity   189.55\n",
            "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 765.60 | loss  5.15 | perplexity   173.00\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 280.33s| valid loss  5.41 | valid perplexity   223.69\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 |    25/  363 batches | lr 20.00 | ms/batch 782.87 | loss  4.99 | perplexity   147.15\n",
            "| epoch   2 |    50/  363 batches | lr 20.00 | ms/batch 747.43 | loss  4.67 | perplexity   107.23\n",
            "| epoch   2 |    75/  363 batches | lr 20.00 | ms/batch 739.32 | loss  4.70 | perplexity   110.28\n",
            "| epoch   2 |   100/  363 batches | lr 20.00 | ms/batch 755.56 | loss  4.68 | perplexity   107.99\n",
            "| epoch   2 |   125/  363 batches | lr 20.00 | ms/batch 738.74 | loss  4.61 | perplexity   100.66\n",
            "| epoch   2 |   150/  363 batches | lr 20.00 | ms/batch 751.83 | loss  4.49 | perplexity    89.53\n",
            "| epoch   2 |   175/  363 batches | lr 20.00 | ms/batch 743.20 | loss  4.59 | perplexity    98.72\n",
            "| epoch   2 |   200/  363 batches | lr 20.00 | ms/batch 738.55 | loss  4.71 | perplexity   110.58\n",
            "| epoch   2 |   225/  363 batches | lr 20.00 | ms/batch 765.27 | loss  5.02 | perplexity   151.47\n",
            "| epoch   2 |   250/  363 batches | lr 20.00 | ms/batch 735.24 | loss  4.99 | perplexity   146.96\n",
            "| epoch   2 |   275/  363 batches | lr 20.00 | ms/batch 747.32 | loss  5.03 | perplexity   153.37\n",
            "| epoch   2 |   300/  363 batches | lr 20.00 | ms/batch 738.89 | loss  5.01 | perplexity   150.33\n",
            "| epoch   2 |   325/  363 batches | lr 20.00 | ms/batch 749.41 | loss  5.10 | perplexity   164.35\n",
            "| epoch   2 |   350/  363 batches | lr 20.00 | ms/batch 744.62 | loss  5.02 | perplexity   152.15\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 280.10s| valid loss  5.42 | valid perplexity   225.06\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   3 |    25/  363 batches | lr 20.00 | ms/batch 778.46 | loss  4.94 | perplexity   139.56\n",
            "| epoch   3 |    50/  363 batches | lr 20.00 | ms/batch 731.78 | loss  4.62 | perplexity   101.13\n",
            "| epoch   3 |    75/  363 batches | lr 20.00 | ms/batch 752.38 | loss  4.64 | perplexity   103.51\n",
            "| epoch   3 |   100/  363 batches | lr 20.00 | ms/batch 762.27 | loss  4.62 | perplexity   101.10\n",
            "| epoch   3 |   125/  363 batches | lr 20.00 | ms/batch 748.20 | loss  4.56 | perplexity    95.44\n",
            "| epoch   3 |   150/  363 batches | lr 20.00 | ms/batch 748.57 | loss  4.45 | perplexity    85.29\n",
            "| epoch   3 |   175/  363 batches | lr 20.00 | ms/batch 735.17 | loss  4.54 | perplexity    93.24\n",
            "| epoch   3 |   200/  363 batches | lr 20.00 | ms/batch 747.80 | loss  4.64 | perplexity   103.98\n",
            "| epoch   3 |   225/  363 batches | lr 20.00 | ms/batch 738.41 | loss  4.90 | perplexity   134.17\n",
            "| epoch   3 |   250/  363 batches | lr 20.00 | ms/batch 748.78 | loss  4.87 | perplexity   130.89\n",
            "| epoch   3 |   275/  363 batches | lr 20.00 | ms/batch 751.07 | loss  4.92 | perplexity   137.39\n",
            "| epoch   3 |   300/  363 batches | lr 20.00 | ms/batch 738.07 | loss  4.91 | perplexity   135.28\n",
            "| epoch   3 |   325/  363 batches | lr 20.00 | ms/batch 747.37 | loss  5.00 | perplexity   147.79\n",
            "| epoch   3 |   350/  363 batches | lr 20.00 | ms/batch 734.63 | loss  4.92 | perplexity   137.18\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 280.19s| valid loss  5.42 | valid perplexity   224.86\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   4 |    25/  363 batches | lr 20.00 | ms/batch 764.04 | loss  4.88 | perplexity   131.17\n",
            "| epoch   4 |    50/  363 batches | lr 20.00 | ms/batch 752.14 | loss  4.55 | perplexity    94.99\n",
            "| epoch   4 |    75/  363 batches | lr 20.00 | ms/batch 752.26 | loss  4.58 | perplexity    97.73\n",
            "| epoch   4 |   100/  363 batches | lr 20.00 | ms/batch 733.78 | loss  4.57 | perplexity    96.13\n",
            "| epoch   4 |   125/  363 batches | lr 20.00 | ms/batch 749.21 | loss  4.51 | perplexity    90.76\n",
            "| epoch   4 |   150/  363 batches | lr 20.00 | ms/batch 734.81 | loss  4.40 | perplexity    81.41\n",
            "| epoch   4 |   175/  363 batches | lr 20.00 | ms/batch 748.95 | loss  4.49 | perplexity    88.88\n",
            "| epoch   4 |   200/  363 batches | lr 20.00 | ms/batch 754.70 | loss  4.59 | perplexity    98.32\n",
            "| epoch   4 |   225/  363 batches | lr 20.00 | ms/batch 740.68 | loss  4.81 | perplexity   122.45\n",
            "| epoch   4 |   250/  363 batches | lr 20.00 | ms/batch 765.96 | loss  4.78 | perplexity   119.49\n",
            "| epoch   4 |   275/  363 batches | lr 20.00 | ms/batch 727.97 | loss  4.84 | perplexity   126.29\n",
            "| epoch   4 |   300/  363 batches | lr 20.00 | ms/batch 746.57 | loss  4.82 | perplexity   123.69\n",
            "| epoch   4 |   325/  363 batches | lr 20.00 | ms/batch 737.25 | loss  4.90 | perplexity   134.48\n",
            "| epoch   4 |   350/  363 batches | lr 20.00 | ms/batch 750.16 | loss  4.84 | perplexity   125.88\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 279.95s| valid loss  5.44 | valid perplexity   230.47\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 774.18 | loss  5.93 | perplexity   376.36\n",
            "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 728.21 | loss  5.68 | perplexity   294.33\n",
            "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 744.57 | loss  5.67 | perplexity   289.94\n",
            "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 733.91 | loss  5.67 | perplexity   290.16\n",
            "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 744.63 | loss  5.62 | perplexity   275.92\n",
            "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 746.81 | loss  5.50 | perplexity   244.63\n",
            "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 730.40 | loss  5.63 | perplexity   278.00\n",
            "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 742.07 | loss  5.73 | perplexity   307.74\n",
            "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 742.79 | loss  5.91 | perplexity   369.30\n",
            "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 748.64 | loss  5.85 | perplexity   347.85\n",
            "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 735.10 | loss  5.88 | perplexity   359.46\n",
            "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 744.47 | loss  5.86 | perplexity   349.37\n",
            "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 747.74 | loss  5.92 | perplexity   372.21\n",
            "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 732.54 | loss  5.83 | perplexity   341.54\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   0 | time: 278.00s| valid loss  5.84 | valid perplexity   344.11\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 751.90 | loss  5.96 | perplexity   386.46\n",
            "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 744.50 | loss  5.67 | perplexity   289.31\n",
            "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 731.55 | loss  5.65 | perplexity   284.11\n",
            "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 753.59 | loss  5.65 | perplexity   284.50\n",
            "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 745.95 | loss  5.59 | perplexity   268.03\n",
            "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 729.70 | loss  5.47 | perplexity   238.35\n",
            "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 742.34 | loss  5.60 | perplexity   270.01\n",
            "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 723.29 | loss  5.68 | perplexity   293.37\n",
            "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 740.91 | loss  5.79 | perplexity   327.60\n",
            "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 723.98 | loss  5.75 | perplexity   313.16\n",
            "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 740.29 | loss  5.77 | perplexity   322.03\n",
            "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 743.83 | loss  5.76 | perplexity   316.76\n",
            "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 726.09 | loss  5.83 | perplexity   340.30\n",
            "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 738.46 | loss  5.75 | perplexity   313.86\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 276.47s| valid loss  5.80 | valid perplexity   328.74\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 |    25/  363 batches | lr 20.00 | ms/batch 772.49 | loss  5.90 | perplexity   363.66\n",
            "| epoch   2 |    50/  363 batches | lr 20.00 | ms/batch 732.14 | loss  5.61 | perplexity   272.60\n",
            "| epoch   2 |    75/  363 batches | lr 20.00 | ms/batch 742.71 | loss  5.60 | perplexity   269.59\n",
            "| epoch   2 |   100/  363 batches | lr 20.00 | ms/batch 739.46 | loss  5.60 | perplexity   270.84\n",
            "| epoch   2 |   125/  363 batches | lr 20.00 | ms/batch 733.51 | loss  5.54 | perplexity   254.97\n",
            "| epoch   2 |   150/  363 batches | lr 20.00 | ms/batch 742.71 | loss  5.42 | perplexity   225.77\n",
            "| epoch   2 |   175/  363 batches | lr 20.00 | ms/batch 731.43 | loss  5.54 | perplexity   255.77\n",
            "| epoch   2 |   200/  363 batches | lr 20.00 | ms/batch 747.83 | loss  5.62 | perplexity   276.71\n",
            "| epoch   2 |   225/  363 batches | lr 20.00 | ms/batch 732.29 | loss  5.70 | perplexity   298.90\n",
            "| epoch   2 |   250/  363 batches | lr 20.00 | ms/batch 741.89 | loss  5.67 | perplexity   289.01\n",
            "| epoch   2 |   275/  363 batches | lr 20.00 | ms/batch 752.79 | loss  5.70 | perplexity   298.41\n",
            "| epoch   2 |   300/  363 batches | lr 20.00 | ms/batch 727.85 | loss  5.68 | perplexity   293.59\n",
            "| epoch   2 |   325/  363 batches | lr 20.00 | ms/batch 743.26 | loss  5.76 | perplexity   318.56\n",
            "| epoch   2 |   350/  363 batches | lr 20.00 | ms/batch 732.20 | loss  5.68 | perplexity   292.99\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 277.54s| valid loss  5.77 | valid perplexity   320.87\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   3 |    25/  363 batches | lr 20.00 | ms/batch 754.95 | loss  5.85 | perplexity   346.98\n",
            "| epoch   3 |    50/  363 batches | lr 20.00 | ms/batch 739.84 | loss  5.56 | perplexity   259.07\n",
            "| epoch   3 |    75/  363 batches | lr 20.00 | ms/batch 733.29 | loss  5.55 | perplexity   257.77\n",
            "| epoch   3 |   100/  363 batches | lr 20.00 | ms/batch 731.52 | loss  5.56 | perplexity   259.77\n",
            "| epoch   3 |   125/  363 batches | lr 20.00 | ms/batch 744.36 | loss  5.50 | perplexity   244.14\n",
            "| epoch   3 |   150/  363 batches | lr 20.00 | ms/batch 729.24 | loss  5.38 | perplexity   216.33\n",
            "| epoch   3 |   175/  363 batches | lr 20.00 | ms/batch 756.96 | loss  5.50 | perplexity   244.67\n",
            "| epoch   3 |   200/  363 batches | lr 20.00 | ms/batch 724.27 | loss  5.59 | perplexity   266.49\n",
            "| epoch   3 |   225/  363 batches | lr 20.00 | ms/batch 736.88 | loss  5.64 | perplexity   281.40\n",
            "| epoch   3 |   250/  363 batches | lr 20.00 | ms/batch 726.70 | loss  5.61 | perplexity   272.00\n",
            "| epoch   3 |   275/  363 batches | lr 20.00 | ms/batch 740.17 | loss  5.64 | perplexity   282.81\n",
            "| epoch   3 |   300/  363 batches | lr 20.00 | ms/batch 745.42 | loss  5.63 | perplexity   279.15\n",
            "| epoch   3 |   325/  363 batches | lr 20.00 | ms/batch 730.82 | loss  5.70 | perplexity   300.25\n",
            "| epoch   3 |   350/  363 batches | lr 20.00 | ms/batch 737.42 | loss  5.62 | perplexity   276.87\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 276.03s| valid loss  5.74 | valid perplexity   312.11\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   4 |    25/  363 batches | lr 20.00 | ms/batch 770.01 | loss  5.80 | perplexity   331.26\n",
            "| epoch   4 |    50/  363 batches | lr 20.00 | ms/batch 722.87 | loss  5.51 | perplexity   247.48\n",
            "| epoch   4 |    75/  363 batches | lr 20.00 | ms/batch 758.56 | loss  5.51 | perplexity   247.77\n",
            "| epoch   4 |   100/  363 batches | lr 20.00 | ms/batch 745.01 | loss  5.51 | perplexity   247.91\n",
            "| epoch   4 |   125/  363 batches | lr 20.00 | ms/batch 727.90 | loss  5.46 | perplexity   235.12\n",
            "| epoch   4 |   150/  363 batches | lr 20.00 | ms/batch 742.97 | loss  5.33 | perplexity   207.20\n",
            "| epoch   4 |   175/  363 batches | lr 20.00 | ms/batch 728.16 | loss  5.46 | perplexity   234.62\n",
            "| epoch   4 |   200/  363 batches | lr 20.00 | ms/batch 741.39 | loss  5.55 | perplexity   256.35\n",
            "| epoch   4 |   225/  363 batches | lr 20.00 | ms/batch 727.44 | loss  5.59 | perplexity   269.03\n",
            "| epoch   4 |   250/  363 batches | lr 20.00 | ms/batch 740.42 | loss  5.56 | perplexity   259.43\n",
            "| epoch   4 |   275/  363 batches | lr 20.00 | ms/batch 738.76 | loss  5.59 | perplexity   268.14\n",
            "| epoch   4 |   300/  363 batches | lr 20.00 | ms/batch 732.34 | loss  5.58 | perplexity   263.97\n",
            "| epoch   4 |   325/  363 batches | lr 20.00 | ms/batch 737.45 | loss  5.65 | perplexity   284.37\n",
            "| epoch   4 |   350/  363 batches | lr 20.00 | ms/batch 740.20 | loss  5.57 | perplexity   262.14\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 277.04s| valid loss  5.72 | valid perplexity   306.15\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 856.41 | loss  5.24 | perplexity   189.09\n",
            "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 844.99 | loss  5.03 | perplexity   152.92\n",
            "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 810.49 | loss  5.00 | perplexity   148.66\n",
            "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 812.87 | loss  5.01 | perplexity   149.66\n",
            "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 817.07 | loss  4.94 | perplexity   139.19\n",
            "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 821.05 | loss  4.80 | perplexity   121.93\n",
            "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 813.03 | loss  4.95 | perplexity   140.57\n",
            "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 837.21 | loss  5.08 | perplexity   160.51\n",
            "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 831.87 | loss  5.30 | perplexity   199.53\n",
            "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 825.50 | loss  5.23 | perplexity   186.69\n",
            "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 814.74 | loss  5.29 | perplexity   199.06\n",
            "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 849.27 | loss  5.24 | perplexity   189.35\n",
            "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 829.56 | loss  5.31 | perplexity   202.03\n",
            "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 829.87 | loss  5.25 | perplexity   191.10\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   0 | time: 309.36s| valid loss  5.26 | valid perplexity   193.01\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 855.62 | loss  5.32 | perplexity   204.01\n",
            "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 819.68 | loss  4.98 | perplexity   146.09\n",
            "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 820.63 | loss  4.97 | perplexity   144.63\n",
            "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 802.85 | loss  4.98 | perplexity   144.81\n",
            "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 819.96 | loss  4.90 | perplexity   134.57\n",
            "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 819.33 | loss  4.76 | perplexity   117.08\n",
            "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 827.80 | loss  4.90 | perplexity   134.39\n",
            "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 810.65 | loss  5.01 | perplexity   149.79\n",
            "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 819.34 | loss  5.12 | perplexity   167.52\n",
            "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 825.05 | loss  5.07 | perplexity   159.33\n",
            "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 812.19 | loss  5.14 | perplexity   170.36\n",
            "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 815.55 | loss  5.08 | perplexity   161.45\n",
            "| epoch   1 |   325/  363 batches | lr 20.00 | ms/batch 817.57 | loss  5.16 | perplexity   173.56\n",
            "| epoch   1 |   350/  363 batches | lr 20.00 | ms/batch 820.60 | loss  5.10 | perplexity   163.33\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 306.74s| valid loss  5.23 | valid perplexity   187.29\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   2 |    25/  363 batches | lr 20.00 | ms/batch 855.75 | loss  5.23 | perplexity   186.41\n",
            "| epoch   2 |    50/  363 batches | lr 20.00 | ms/batch 838.53 | loss  4.90 | perplexity   134.48\n",
            "| epoch   2 |    75/  363 batches | lr 20.00 | ms/batch 816.34 | loss  4.91 | perplexity   134.99\n",
            "| epoch   2 |   100/  363 batches | lr 20.00 | ms/batch 804.63 | loss  4.91 | perplexity   135.35\n",
            "| epoch   2 |   125/  363 batches | lr 20.00 | ms/batch 822.28 | loss  4.83 | perplexity   124.59\n",
            "| epoch   2 |   150/  363 batches | lr 20.00 | ms/batch 822.03 | loss  4.68 | perplexity   107.91\n",
            "| epoch   2 |   175/  363 batches | lr 20.00 | ms/batch 818.15 | loss  4.82 | perplexity   124.29\n",
            "| epoch   2 |   200/  363 batches | lr 20.00 | ms/batch 814.94 | loss  4.93 | perplexity   138.68\n",
            "| epoch   2 |   225/  363 batches | lr 20.00 | ms/batch 818.89 | loss  5.00 | perplexity   148.03\n",
            "| epoch   2 |   250/  363 batches | lr 20.00 | ms/batch 825.67 | loss  4.95 | perplexity   141.81\n",
            "| epoch   2 |   275/  363 batches | lr 20.00 | ms/batch 826.46 | loss  5.02 | perplexity   151.11\n",
            "| epoch   2 |   300/  363 batches | lr 20.00 | ms/batch 829.46 | loss  4.98 | perplexity   146.16\n",
            "| epoch   2 |   325/  363 batches | lr 20.00 | ms/batch 821.97 | loss  5.05 | perplexity   155.25\n",
            "| epoch   2 |   350/  363 batches | lr 20.00 | ms/batch 819.46 | loss  5.00 | perplexity   148.26\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 307.89s| valid loss  5.20 | valid perplexity   180.59\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   3 |    25/  363 batches | lr 20.00 | ms/batch 846.06 | loss  5.17 | perplexity   176.56\n",
            "| epoch   3 |    50/  363 batches | lr 20.00 | ms/batch 820.64 | loss  4.83 | perplexity   125.79\n",
            "| epoch   3 |    75/  363 batches | lr 20.00 | ms/batch 818.32 | loss  4.84 | perplexity   126.63\n",
            "| epoch   3 |   100/  363 batches | lr 20.00 | ms/batch 805.91 | loss  4.85 | perplexity   127.37\n",
            "| epoch   3 |   125/  363 batches | lr 20.00 | ms/batch 820.59 | loss  4.76 | perplexity   117.23\n",
            "| epoch   3 |   150/  363 batches | lr 20.00 | ms/batch 819.34 | loss  4.62 | perplexity   101.37\n",
            "| epoch   3 |   175/  363 batches | lr 20.00 | ms/batch 837.17 | loss  4.76 | perplexity   116.93\n",
            "| epoch   3 |   200/  363 batches | lr 20.00 | ms/batch 807.20 | loss  4.88 | perplexity   131.96\n",
            "| epoch   3 |   225/  363 batches | lr 20.00 | ms/batch 820.38 | loss  4.91 | perplexity   136.15\n",
            "| epoch   3 |   250/  363 batches | lr 20.00 | ms/batch 823.18 | loss  4.87 | perplexity   130.25\n",
            "| epoch   3 |   275/  363 batches | lr 20.00 | ms/batch 824.20 | loss  4.94 | perplexity   139.56\n",
            "| epoch   3 |   300/  363 batches | lr 20.00 | ms/batch 807.31 | loss  4.91 | perplexity   135.29\n",
            "| epoch   3 |   325/  363 batches | lr 20.00 | ms/batch 823.30 | loss  4.97 | perplexity   144.29\n",
            "| epoch   3 |   350/  363 batches | lr 20.00 | ms/batch 825.44 | loss  4.92 | perplexity   137.14\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 307.16s| valid loss  5.19 | valid perplexity   179.11\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   4 |    25/  363 batches | lr 20.00 | ms/batch 852.87 | loss  5.11 | perplexity   165.81\n",
            "| epoch   4 |    50/  363 batches | lr 20.00 | ms/batch 844.35 | loss  4.77 | perplexity   117.72\n",
            "| epoch   4 |    75/  363 batches | lr 20.00 | ms/batch 818.57 | loss  4.78 | perplexity   119.26\n",
            "| epoch   4 |   100/  363 batches | lr 20.00 | ms/batch 806.16 | loss  4.79 | perplexity   119.73\n",
            "| epoch   4 |   125/  363 batches | lr 20.00 | ms/batch 823.43 | loss  4.71 | perplexity   110.57\n",
            "| epoch   4 |   150/  363 batches | lr 20.00 | ms/batch 820.12 | loss  4.56 | perplexity    95.34\n",
            "| epoch   4 |   175/  363 batches | lr 20.00 | ms/batch 817.79 | loss  4.70 | perplexity   110.47\n",
            "| epoch   4 |   200/  363 batches | lr 20.00 | ms/batch 806.59 | loss  4.82 | perplexity   123.56\n",
            "| epoch   4 |   225/  363 batches | lr 20.00 | ms/batch 821.85 | loss  4.85 | perplexity   127.23\n",
            "| epoch   4 |   250/  363 batches | lr 20.00 | ms/batch 822.62 | loss  4.80 | perplexity   122.09\n",
            "| epoch   4 |   275/  363 batches | lr 20.00 | ms/batch 825.44 | loss  4.87 | perplexity   130.65\n",
            "| epoch   4 |   300/  363 batches | lr 20.00 | ms/batch 833.50 | loss  4.84 | perplexity   126.34\n",
            "| epoch   4 |   325/  363 batches | lr 20.00 | ms/batch 821.27 | loss  4.89 | perplexity   133.44\n",
            "| epoch   4 |   350/  363 batches | lr 20.00 | ms/batch 822.19 | loss  4.86 | perplexity   128.96\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 308.10s| valid loss  5.14 | valid perplexity   171.55\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   0 |    25/  363 batches | lr 20.00 | ms/batch 867.54 | loss  5.56 | perplexity   258.93\n",
            "| epoch   0 |    50/  363 batches | lr 20.00 | ms/batch 802.88 | loss  5.34 | perplexity   208.30\n",
            "| epoch   0 |    75/  363 batches | lr 20.00 | ms/batch 822.98 | loss  5.32 | perplexity   203.44\n",
            "| epoch   0 |   100/  363 batches | lr 20.00 | ms/batch 818.86 | loss  5.31 | perplexity   201.93\n",
            "| epoch   0 |   125/  363 batches | lr 20.00 | ms/batch 824.18 | loss  5.23 | perplexity   187.64\n",
            "| epoch   0 |   150/  363 batches | lr 20.00 | ms/batch 803.31 | loss  5.11 | perplexity   165.14\n",
            "| epoch   0 |   175/  363 batches | lr 20.00 | ms/batch 821.97 | loss  5.23 | perplexity   187.04\n",
            "| epoch   0 |   200/  363 batches | lr 20.00 | ms/batch 817.49 | loss  5.36 | perplexity   211.78\n",
            "| epoch   0 |   225/  363 batches | lr 20.00 | ms/batch 814.49 | loss  5.50 | perplexity   245.67\n",
            "| epoch   0 |   250/  363 batches | lr 20.00 | ms/batch 806.06 | loss  5.44 | perplexity   231.06\n",
            "| epoch   0 |   275/  363 batches | lr 20.00 | ms/batch 838.23 | loss  5.50 | perplexity   243.78\n",
            "| epoch   0 |   300/  363 batches | lr 20.00 | ms/batch 819.16 | loss  5.46 | perplexity   234.11\n",
            "| epoch   0 |   325/  363 batches | lr 20.00 | ms/batch 806.45 | loss  5.52 | perplexity   249.58\n",
            "| epoch   0 |   350/  363 batches | lr 20.00 | ms/batch 817.20 | loss  5.43 | perplexity   228.96\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| end of epoch   0 | time: 306.70s| valid loss  5.46 | valid perplexity   235.97\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| epoch   1 |    25/  363 batches | lr 20.00 | ms/batch 848.41 | loss  5.57 | perplexity   262.73\n",
            "| epoch   1 |    50/  363 batches | lr 20.00 | ms/batch 801.71 | loss  5.28 | perplexity   195.62\n",
            "| epoch   1 |    75/  363 batches | lr 20.00 | ms/batch 815.62 | loss  5.25 | perplexity   190.68\n",
            "| epoch   1 |   100/  363 batches | lr 20.00 | ms/batch 820.66 | loss  5.25 | perplexity   190.81\n",
            "| epoch   1 |   125/  363 batches | lr 20.00 | ms/batch 819.96 | loss  5.17 | perplexity   176.53\n",
            "| epoch   1 |   150/  363 batches | lr 20.00 | ms/batch 808.86 | loss  5.05 | perplexity   155.78\n",
            "| epoch   1 |   175/  363 batches | lr 20.00 | ms/batch 841.46 | loss  5.17 | perplexity   175.94\n",
            "| epoch   1 |   200/  363 batches | lr 20.00 | ms/batch 815.37 | loss  5.28 | perplexity   196.19\n",
            "| epoch   1 |   225/  363 batches | lr 20.00 | ms/batch 809.87 | loss  5.35 | perplexity   210.48\n",
            "| epoch   1 |   250/  363 batches | lr 20.00 | ms/batch 810.18 | loss  5.30 | perplexity   200.21\n",
            "| epoch   1 |   275/  363 batches | lr 20.00 | ms/batch 814.56 | loss  5.36 | perplexity   212.02\n",
            "| epoch   1 |   300/  363 batches | lr 20.00 | ms/batch 819.58 | loss  5.33 | perplexity   205.58\n"
          ]
        }
      ],
      "source": [
        "CUT_AFTER_BATCHES = -1\n",
        "\n",
        "\n",
        "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, dictionary: u5.Dictionary,\n",
        "          max_seq_len: int, train_batch_size: int, train_data_splits,\n",
        "          clipping: float, learning_rate: float, print_interval: int, epoch: int,\n",
        "          criterion: torch.nn.Module = torch.nn.NLLLoss()):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    ntokens = len(dictionary)\n",
        "    start_hidden = None\n",
        "    n_batches = (train_data_splits.size(0) - 1) // max_seq_len\n",
        "    \n",
        "    for batch_i, i in enumerate(range(0, train_data_splits.size(0) - 1, max_seq_len)):\n",
        "        batch_data, batch_targets = u5.get_batch(train_data_splits, i, max_seq_len)\n",
        "        # ipdb.set_trace()\n",
        "      \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if start_hidden is not None:\n",
        "            start_hidden = u5.repackage_hidden(start_hidden)\n",
        "        \n",
        "        # Forward pass\n",
        "        y_hat_logprobs, last_hidden = model(batch_data, start_hidden, return_logs=True)\n",
        "        \n",
        "        # Loss computation & backward pass\n",
        "        y_hat_logprobs = y_hat_logprobs.view(-1, ntokens)\n",
        "        loss = criterion(y_hat_logprobs, batch_targets.view(-1))\n",
        "        loss.backward()\n",
        "        \n",
        "        start_hidden = last_hidden\n",
        "        \n",
        "        # Clipping gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
        "        \n",
        "        # Updating parameters using SGD\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        if batch_i % print_interval == 0 and batch_i > 0:\n",
        "            cur_loss = total_loss / print_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            throughput = elapsed * 1000 / print_interval\n",
        "            print(f\"| epoch {epoch:3d} | {batch_i:5d}/{n_batches:5d} batches | lr {learning_rate:02.2f} | ms/batch {throughput:5.2f} \"\n",
        "                  f\"| loss {cur_loss:5.2f} | perplexity {math.exp(cur_loss):8.2f}\")\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        \n",
        "        # Cuts the loop (only for debugging)\n",
        "        if (CUT_AFTER_BATCHES != -1) and (batch_i >= CUT_AFTER_BATCHES):\n",
        "            print(f\"WARNING: Training is interrupted after {batch_i} batches\")\n",
        "            break\n",
        "            \n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "\n",
        "epochs = 5  # total number of training epochs\n",
        "print_interval = 25  # print report statistics every x batches\n",
        "lr = 20  # initial learning rate\n",
        "clipping = 0.25  # gradient clipping\n",
        "models = [model_1layer_no_tying, model_1layer_tying, model_2layers_no_tying, model_2layers_tying]\n",
        "for model in models:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    best_val_loss = None\n",
        "\n",
        "    # Loop over epochs.\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
        "        val_loss = u5.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
        "              f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save_path, \"wb\") as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "    # Evaluate the model on the train, validation, and test sets\n",
        "    train_loss = u5.evaluate(model, dictionary, max_seq_len, eval_batch_size, train_data_splits)\n",
        "    val_loss = u5.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
        "    test_loss = u5.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_splits)\n",
        "\n",
        "    # Append the loss values to the corresponding lists\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "# Create the plot to visualize the loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "epochs = range(1, epochs + 1)  # Use the fixed number of epochs\n",
        "\n",
        "# Plot loss on each current training batch before every model update step as a function of epochs\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss on Training Batches')\n",
        "\n",
        "# Plot loss on the validation set at every epoch\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, val_losses, label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss on Validation Set')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9m8hmKZqXuF1"
      },
      "source": [
        "your answer goes here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1626IIR_XuF2"
      },
      "source": [
        "<h2>Language Generation</h2>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TI4Rfj7YXuF3"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <b>Exercise 4. [30 Points]</b>\n",
        "    <p>\n",
        "    Copy the language generation code from the main exercise notebook and perform the following tasks:\n",
        "    </p>\n",
        "        <ul>\n",
        "            <li>Compare all four previous models by generating $12$ words that append the starting word <tt>\"despite\"</tt>.</li>\n",
        "            <li>For each model, retrieve the top $10$ wordIDs with the highest probabilities from the generated probability distribution (<code>prob_dist</code>) following the starting word <tt>\"despite\"</tt>. Fetch the corresponding words of these wordIDs. Do you observe any specific linguistic characteristic common between these words?</li>\n",
        "            <li>The implementation in the main exercise notebook is based on sampling. Implement a second deterministic variant based on the <i>top-1</i> approach. In this particular variant, the generated word is the word with the highest probability in the predicted probability distribution. Repeat the same procedure as before (i.e., generate $12$ words that append the starting word <tt>\"despite\"</tt>).</li>\n",
        "        </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q01sZx_cXuF4",
        "outputId": "03b4d526-769e-4aa9-c953-cf66fcd08a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: LM_LSTMModel(\n",
            "  (encoder): Embedding(10001, 200)\n",
            "  (rnn): LSTM(200, 200)\n",
            "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
            ")\n",
            "Generated Text: despite colleagues master houses buildings debris repurchase ind. vowed rocked interbank saw application\n",
            "Top 10 Words: ['application', 'scorpio', 'seemingly', 'response', 'soda', 'eastman', 'colleagues', 'surrounding', 'average', 'guaranteed']\n",
            "\n",
            "Model: LM_LSTMModel(\n",
            "  (encoder): Embedding(10001, 200)\n",
            "  (rnn): LSTM(200, 200)\n",
            "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
            ")\n",
            "Generated Text: despite expression rank saab marketplace democracy page-one spends oversight competes sweat investigate staffers\n",
            "Top 10 Words: ['staffers', 'hell', 'values', 'legislators', \"n't\", 'credibility', 'squeeze', 'dalkon', 'confidence', 'redemption']\n",
            "\n",
            "Model: LM_LSTMModel(\n",
            "  (encoder): Embedding(10001, 200)\n",
            "  (rnn): LSTM(200, 200, num_layers=2)\n",
            "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
            ")\n",
            "Generated Text: despite pretax sutton refer murphy murphy murphy neb. neb. neb. rescue rescue rescue\n",
            "Top 10 Words: ['rescue', 'stance', 'profitably', 'cat', 'declining', 'brouwer', 'borrowers', 'picop', 'tripled', 'guarantee']\n",
            "\n",
            "Model: LM_LSTMModel(\n",
            "  (encoder): Embedding(10001, 200)\n",
            "  (rnn): LSTM(200, 200, num_layers=2)\n",
            "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
            ")\n",
            "Generated Text: despite chorus demands restoring restoring resulted jr jr peat thailand thailand hartford hands\n",
            "Top 10 Words: ['hands', 'montedison', 'esb', 'opec', 'ethiopia', 'irish', 'five-cent', 'followed', 'carol', 'respect']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "GENERATION_LENGTH = 12\n",
        "START_WORD = \"despite\"\n",
        "\n",
        "models = [model_1layer_no_tying, model_1layer_tying, model_2layers_no_tying, model_2layers_tying]\n",
        "\n",
        "for model in models:\n",
        "    start_hidden = None\n",
        "    START_WORD = START_WORD.lower()\n",
        "\n",
        "    generated_text = START_WORD\n",
        "    with torch.no_grad():\n",
        "        wordid_input = dictionary.word2idx[START_WORD]\n",
        "        for i in range(0, GENERATION_LENGTH):\n",
        "            data = u5.batchify(torch.tensor([wordid_input]), 1, device)\n",
        "\n",
        "            y_hat_probs, last_hidden = model(data, start_hidden, return_logs=False)\n",
        "\n",
        "            top_probs, top_indices = torch.topk(y_hat_probs.squeeze(), k=10, dim=-1)\n",
        "            top_words = [dictionary.idx2word[idx.item()] for idx in top_indices]\n",
        "\n",
        "            wordid_input = torch.argmax(y_hat_probs.squeeze(), dim=-1)\n",
        "            word_generated = dictionary.idx2word[wordid_input.item()]\n",
        "\n",
        "            generated_text += \" \" + word_generated\n",
        "\n",
        "            start_hidden = last_hidden\n",
        "\n",
        "    print(f\"Model: {model}\")\n",
        "    print(\"Generated Text:\", generated_text)\n",
        "    print(\"Top 10 Words:\", top_words)\n",
        "    print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BR5vQ9ToXuF5"
      },
      "source": [
        "The word \"<unk>\" (unknown) appears frequently in the generated text and is also present in the top 10 words.\n",
        "\n",
        "Common words such as \"and\", \"in\", \"to\", \"for\", \"is\", \"on\", \"the\", \"of\", and \"or\" appear in the top 10 words for multiple models. These words are general.\n",
        "\n",
        "The word \"market\" appears in the top 10 words for two models. This suggests that the models might have learned some association between the starting word \"despite\" and the concept of \"market\"."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
